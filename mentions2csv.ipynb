{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLqTzEUTqfDbkL8ioK9kEF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/debrrox/impact_analysis/blob/read_emails/mentions2csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YY2UBEpkkgH",
        "outputId": "3af751fa-462e-43bc-b485-8186050c5ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download xx_ent_wiki_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNDGVwZqkvDF",
        "outputId": "e50df54d-853d-4898-c689-c3cd0af876b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xx-ent-wiki-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/xx_ent_wiki_sm-3.7.0/xx_ent_wiki_sm-3.7.0-py3-none-any.whl (11.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from xx-ent-wiki-sm==3.7.0) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.12.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->xx-ent-wiki-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('xx_ent_wiki_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import email\n",
        "from email import policy\n",
        "from email.parser import BytesParser\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "import pandas as pd\n",
        "import json\n",
        "import langdetect\n",
        "import spacy\n",
        "import numpy as np\n",
        "import urllib\n",
        "from urllib.parse import urlparse\n"
      ],
      "metadata": {
        "id": "HGSy2EglkLM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDriUL-ynRHs",
        "outputId": "f1df2c74-d2b1-4a4c-b679-91dbcc12f00e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "COLUMNS = ['Title', 'Source', 'Summary', 'Date', 'State', 'Website', 'Language', 'NER', 'G/T']\n",
        "SPACY_NLP = spacy.load(\"xx_ent_wiki_sm\")"
      ],
      "metadata": {
        "id": "gOEDZgV2kNGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_links_from_google(eml_file_path):\n",
        "    with open(eml_file_path, 'r') as file:\n",
        "        # Parse the eml file\n",
        "        eml_content = file.read()\n",
        "    json_match = re.search(r'application/json\">(.*?)</script>', eml_content, re.DOTALL)\n",
        "    data = []\n",
        "    if json_match:\n",
        "        json_str = json_match.group(1)\n",
        "        # Fix any escaped characters\n",
        "        json_str = json_str.replace('=\\n', '').replace('=3D', '=').replace('\\n', '')\n",
        "        # Load the JSON into a dictionary\n",
        "        data_dict = json.loads(json_str)[\"cards\"]\n",
        "        refs = [elmt[\"widgets\"] for elmt in data_dict]\n",
        "        if refs:\n",
        "            refs = refs[0]\n",
        "        for ref in refs:\n",
        "            title = ref[\"title\"]\n",
        "            parsed_url = urllib.parse.urlparse(ref[\"url\"])\n",
        "            # Extract the query parameters\n",
        "            query_params = urllib.parse.parse_qs(parsed_url.query)\n",
        "            # Get the real URL from the 'url' parameter\n",
        "            source = query_params.get('url', [None])[0]\n",
        "            summary = urllib.parse.unquote(ref[\"description\"].replace('=', '%') , 'utf-8')\n",
        "            website = urlparse(source).netloc.replace(\"www.\", \"\")\n",
        "            lang_sum = langdetect.detect(summary)\n",
        "            ner = detect_NER(summary)\n",
        "            # TODO figure out date\n",
        "            # TODO Add state??\n",
        "            data.append([title, source, summary, \"\", \"\", website, lang_sum, ner, \"G\"])\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data, columns=COLUMNS)\n",
        "    return df"
      ],
      "metadata": {
        "id": "x2GPgLgMeSFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_links_from_talkwalker(eml_file_path):\n",
        "    with open(eml_file_path, 'rb') as file:\n",
        "        # Parse the eml file\n",
        "        msg = BytesParser(policy=policy.default).parse(file)\n",
        "    html_content = msg.get_body(preferencelist=('plain', 'html')).get_content()\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Find the NEWS and TWITTER comments\n",
        "    comments = soup.find_all(string=lambda text: isinstance(text, Comment))\n",
        "    news_index = None\n",
        "    twitter_index = None\n",
        "\n",
        "    for i, comment in enumerate(comments):\n",
        "        if 'NEWS' in comment:\n",
        "            news_index = i\n",
        "        if 'TWITTER' in comment:\n",
        "            twitter_index = i\n",
        "\n",
        "    # Extract the content between NEWS and TWITTER\n",
        "    data = []\n",
        "    if news_index and twitter_index:\n",
        "        news_content = comments[news_index].find_next_sibling()\n",
        "        elements = []\n",
        "        while news_content and news_content != comments[twitter_index]:\n",
        "            elements.append(news_content)\n",
        "            news_content = news_content.find_next_sibling()\n",
        "\n",
        "        # Prepare data for DataFrame\n",
        "        for element in elements:\n",
        "            if element.name == 'tr':\n",
        "                title = element.find('a').text if element.find('a') else None\n",
        "                source = element.find('a')['href'] if element.find('a') else None\n",
        "                summary = None\n",
        "                date = None\n",
        "                state = None\n",
        "                website = None\n",
        "                lang_sum = None\n",
        "                ner = None\n",
        "                for td in element.find_all('td'):\n",
        "                    date_state_website = td.text.strip().split(' | ')\n",
        "                    if td.text.strip()[:3]==\"...\"  or  td.text.strip()[-3:]==\"...\" :\n",
        "                        summary = td.text.replace(\"\\n\", \" \").strip()\n",
        "                        lang_sum = langdetect.detect(summary)\n",
        "                        ner = detect_NER(summary)\n",
        "                    elif len(date_state_website) == 3:\n",
        "                        if len(date_state_website[0]) < 15:\n",
        "                            date = datetime.strptime(date_state_website[0], '%d.%m.%y %H:%M')\n",
        "                            state = date_state_website[1]\n",
        "                            website = date_state_website[2]\n",
        "                if title and \"alerts.talkwalker.com\" not in source and date:\n",
        "                    data.append([title, source, summary, date, state, website, lang_sum, ner, \"T\"])\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(data, columns=COLUMNS)\n",
        "    return df"
      ],
      "metadata": {
        "id": "zffxeqZ-kQBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_NER(text):\n",
        "    nlp = SPACY_NLP\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        if ent.text.lower() == \"disclose\":\n",
        "            return ent.label_\n",
        "    return None"
      ],
      "metadata": {
        "id": "WjFdAVTvkXCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_articles(df):\n",
        "    non_none_cols = ['Title', 'Source', 'Summary', 'NER']\n",
        "    df = df.dropna(subset=non_none_cols)\n",
        "    df = df[~df[\"Website\"].str.contains(\"disclose.ngo\")]\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "t7gyWXsGkXyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_csv(df, output_csv_path):\n",
        "    \"\"\"if os.path.exists(output_csv_path):\n",
        "        df.to_csv(output_csv_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(output_csv_path, mode='w', index=False, header=True)\n",
        "    \"\"\"\n",
        "    df.to_csv(output_csv_path, mode='w', index=False, header=True)\n"
      ],
      "metadata": {
        "id": "4bun6urwkbxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Djsz0wzkG5N"
      },
      "outputs": [],
      "source": [
        "def main():# Call the function and print the extracted links\n",
        "\n",
        "    output_file = \"/content/gdrive/MyDrive/Disclose/alerts.csv\"\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "        #df = pd.read_csv(output_file)\n",
        "    #else:\n",
        "    df = pd.DataFrame(columns=COLUMNS)\n",
        "    input_folder = \"/content/gdrive/MyDrive/Disclose/Emails\"\n",
        "    for eml_file_path in glob.glob(os.path.join(input_folder, \"*\")):\n",
        "        if \"talkwalker\" in eml_file_path:\n",
        "          df_new = extract_links_from_talkwalker(eml_file_path)\n",
        "        else:\n",
        "          df_new = extract_links_from_google(eml_file_path)\n",
        "        df = pd.concat([df, df_new], ignore_index=True)\n",
        "    df.drop_duplicates(inplace=True, ignore_index=True)\n",
        "    df = remove_non_articles(df)\n",
        "    save_csv(df, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "pfGbjsbco8Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61cd53a5-9a44-4070-9101-4417fd4aa247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-da326fa14894>:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  df = pd.concat([df, df_new], ignore_index=True)\n"
          ]
        }
      ]
    }
  ]
}